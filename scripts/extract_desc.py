#!/usr/bin/env python3
"""
EPG Desc æå–å™¨
ä¼˜åŒ–ï¼š
1. ä¿®å¤HTMLå®ä½“ä¹±ç 
2. åªè¿‡æ»¤æ˜ç¡®çš„"æš‚æ— æè¿°"ç±»å ä½æ–‡æœ¬
3. æ™ºèƒ½æ¸…æ´—èŠ‚ç›®å
"""
import os
import sys
import json
import logging
import argparse
import gzip
import re
import html
import xml.etree.ElementTree as ET
from datetime import datetime, timezone, timedelta
import requests

BEIJING_TZ = timezone(timedelta(hours=8))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DescExtractor:
    def __init__(self, config_path, output_path):
        self.config_path = config_path
        self.output_path = output_path
        self.config = None
        
        self.target_channels = {}
        self.desc_db = {}
        
        # æ— æ•ˆdesc - åªè¿‡æ»¤æ˜ç¡®çš„å ä½æ–‡æœ¬
        self.invalid_desc_patterns = [
            r'^æš‚æ— èŠ‚ç›®æè¿°',
            r'^æš‚æ— æè¿°',
            r'^æš‚æ— ç®€ä»‹',
            r'^æš‚æ— å†…å®¹',
            r'^æ— èŠ‚ç›®æè¿°',
            r'^æ— æè¿°',
            r'^æ— ç®€ä»‹',
        ]
        
        self.stats = {
            'sources_processed': 0,
            'total_descs': 0,
            'new_descs': 0,
            'deduplicated': 0,
            'invalid_filtered': 0,
            'html_fixed': 0,
            'channels_with_desc': 0
        }
    
    def normalize(self, text):
        """æ ‡å‡†åŒ–æ–‡æœ¬ç”¨äºç´¢å¼•key"""
        if not text:
            return ""
        text = re.sub(r'[\s\-_\+\|\(\)ï¼ˆï¼‰\[\]ã€ã€‘ã€Šã€‹:ï¼š]', '', text)
        return text.lower()
    
    def fix_html_entities(self, text):
        """
        ä¿®å¤HTMLå®ä½“ä¹±ç 
        å¦‚: &amp;amp;amp;lt;xxx&amp;amp;amp;gt; -> ã€Šxxxã€‹
        """
        if not text:
            return text
        
        original = text
        fixed = text
        
        # å¤šæ¬¡è§£ç ï¼Œå¤„ç†å¤šå±‚è½¬ä¹‰
        for _ in range(10):
            decoded = html.unescape(fixed)
            if decoded == fixed:
                break
            fixed = decoded
        
        # æ›¿æ¢å¸¸è§çš„HTMLæ ‡ç­¾æ®‹ç•™ä¸ºä¸­æ–‡ç¬¦å·
        replacements = [
            ('<', 'ã€Š'),
            ('>', 'ã€‹'),
            ('&lt;', 'ã€Š'),
            ('&gt;', 'ã€‹'),
            ('&quot;', '"'),
            ('&apos;', "'"),
            ('&nbsp;', ' '),
            ('&amp;', '&'),
        ]
        
        for old, new in replacements:
            fixed = fixed.replace(old, new)
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½
        fixed = re.sub(r'\s+', ' ', fixed).strip()
        
        if fixed != original:
            self.stats['html_fixed'] += 1
        
        return fixed
    
    def is_valid_desc(self, desc):
        """
        æ£€æŸ¥descæ˜¯å¦æœ‰æ•ˆ
        åªè¿‡æ»¤æ˜ç¡®çš„"æš‚æ— æè¿°"ç±»å ä½æ–‡æœ¬
        """
        if not desc:
            return False
        
        desc_clean = desc.strip()
        
        # å¤ªçŸ­çš„ä¸è¦ï¼ˆå°‘äº5ä¸ªå­—ç¬¦ï¼‰
        if len(desc_clean) < 5:
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ˜ç¡®çš„å ä½æ–‡æœ¬
        for pattern in self.invalid_desc_patterns:
            if re.match(pattern, desc_clean, re.IGNORECASE):
                self.stats['invalid_filtered'] += 1
                return False
        
        return True
    
    def clean_program_title(self, title):
        """æ¸…æ´—èŠ‚ç›®åç§°"""
        if not title:
            return "", "", False
        
        original = title.strip()
        cleaned = original
        
        # 1. å»é™¤æœ«å°¾çš„æ—¥æœŸæ ¼å¼
        date_patterns = [
            r'\s*[\(ï¼ˆ]?\d{4}[-./å¹´]\d{1,2}[-./æœˆ]\d{1,2}[æ—¥]?[\)ï¼‰]?\s*$',
            r'\s*[\(ï¼ˆ]?\d{8}[\)ï¼‰]?\s*$',
            r'\s*[\(ï¼ˆ]?\d{4}[-./]\d{1,2}[-./]\d{1,2}[\)ï¼‰]?\s*$',
            r'\s*\d{1,2}[-./æœˆ]\d{1,2}[æ—¥]?\s*$',
        ]
        for pattern in date_patterns:
            cleaned = re.sub(pattern, '', cleaned)
        
        # 2. å»é™¤æœŸæ•°/é›†æ•°
        episode_patterns = [
            r'\s*ç¬¬?\d{6,}æœŸ?\s*$',
            r'\s*[\(ï¼ˆ]\d{6,}[\)ï¼‰]\s*$',
            r'\s*ç¬¬\d{1,4}æœŸ\s*$',
            r'\s*ç¬¬\d{1,4}é›†\s*$',
            r'\s*EP?\d{1,4}\s*$',
            r'\s*\(\d{1,4}\)\s*$',
            r'\s*[\(ï¼ˆ]\d{1,3}[\)ï¼‰]\s*$',
            r'\s*ç¬¬\d{1,3}å›\s*$',
        ]
        for pattern in episode_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        # 3. å»é™¤æœ«å°¾å¹´ä»½
        cleaned = re.sub(r'\s*\d{4}\s*$', '', cleaned)
        
        # 4. å»é™¤é‡æ’­/é¦–æ’­æ ‡è®°
        cleaned = re.sub(r'\s*[\(ï¼ˆ]?é‡æ’­[\)ï¼‰]?\s*$', '', cleaned)
        cleaned = re.sub(r'\s*[\(ï¼ˆ]?é¦–æ’­[\)ï¼‰]?\s*$', '', cleaned)
        cleaned = re.sub(r'\s*[\(ï¼ˆ]?ç›´æ’­[\)ï¼‰]?\s*$', '', cleaned)
        
        # 5. å»é™¤æœ«å°¾1-2ä½æ•°å­—
        new_cleaned = re.sub(r'\s*\d{1,2}\s*$', '', cleaned)
        if new_cleaned.strip() and len(new_cleaned) >= 2:
            cleaned = new_cleaned
        
        cleaned = cleaned.strip()
        
        if len(cleaned) < 2:
            cleaned = original
        
        is_cleaned = (cleaned != original)
        
        return cleaned, original, is_cleaned
    
    def load_config(self):
        with open(self.config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        logger.info("é…ç½®åŠ è½½å®Œæˆ")
    
    def download_epg(self, url, compressed=True):
        try:
            if url.startswith(('http://', 'https://')):
                logger.info(f"ä¸‹è½½: {url[:80]}...")
                response = requests.get(url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }, timeout=120)
                response.raise_for_status()
                content = response.content
            else:
                with open(url, 'rb') as f:
                    content = f.read()
            
            if compressed:
                try:
                    content = gzip.decompress(content)
                except:
                    pass
            
            return content
        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            return None
    
    def load_target_channels(self):
        ref_epg = self.config.get('reference_epg')
        if not ref_epg:
            logger.error("é…ç½®ä¸­ç¼ºå°‘ reference_epg")
            return False
        
        content = self.download_epg(ref_epg['url'], ref_epg.get('compressed', True))
        if not content:
            return False
        
        try:
            root = ET.fromstring(content)
            
            for channel in root.findall('.//channel'):
                display_names = channel.findall('display-name')
                if not display_names:
                    continue
                
                first_name = display_names[0].text.strip() if display_names[0].text else None
                if not first_name:
                    continue
                
                for dn in display_names:
                    if dn.text:
                        alias = dn.text.strip()
                        norm_alias = self.normalize(alias)
                        self.target_channels[norm_alias] = first_name
            
            unique_channels = len(set(self.target_channels.values()))
            logger.info(f"ç›®æ ‡é¢‘é“æ•°: {unique_channels}, åˆ«åæ€»æ•°: {len(self.target_channels)}")
            return True
            
        except Exception as e:
            logger.error(f"è§£æå‚è€ƒEPGå¤±è´¥: {e}")
            return False
    
    def get_canonical_channel(self, channel_name):
        norm = self.normalize(channel_name)
        if norm in self.target_channels:
            return self.target_channels[norm], True
        return channel_name, False
    
    def extract_from_source(self, epg_config, source_name):
        content = self.download_epg(epg_config['url'], epg_config.get('compressed', True))
        if not content:
            return
        
        try:
            root = ET.fromstring(content)
            
            channel_map = {}
            for ch in root.findall('.//channel'):
                cid = ch.get('id')
                display_names = ch.findall('display-name')
                if display_names and display_names[0].text:
                    channel_map[cid] = display_names[0].text.strip()
            
            count = 0
            dedup_count = 0
            
            for prog in root.findall('.//programme'):
                cid = prog.get('channel')
                source_channel_name = channel_map.get(cid, '')
                
                canonical_name, is_target = self.get_canonical_channel(source_channel_name)
                if not is_target:
                    continue
                
                title_elem = prog.find('title')
                desc_elem = prog.find('desc')
                
                if title_elem is None or not title_elem.text:
                    continue
                if desc_elem is None or not desc_elem.text:
                    continue
                
                original_title = title_elem.text.strip()
                raw_desc = desc_elem.text.strip()
                
                # ä¿®å¤HTMLå®ä½“ä¹±ç 
                desc = self.fix_html_entities(raw_desc)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ•ˆ
                if not self.is_valid_desc(desc):
                    continue
                
                # æ¸…æ´—èŠ‚ç›®åç§°
                cleaned_title, _, was_cleaned = self.clean_program_title(original_title)
                
                if was_cleaned:
                    dedup_count += 1
                
                norm_channel = self.normalize(canonical_name)
                norm_title = self.normalize(cleaned_title)
                
                if norm_channel not in self.desc_db:
                    self.desc_db[norm_channel] = {}
                
                if norm_title not in self.desc_db[norm_channel]:
                    self.desc_db[norm_channel][norm_title] = {
                        "channel": canonical_name,
                        "title": cleaned_title,
                        "desc": desc
                    }
                    count += 1
                    self.stats['new_descs'] += 1
                elif len(desc) > len(self.desc_db[norm_channel][norm_title]["desc"]):
                    self.desc_db[norm_channel][norm_title]["desc"] = desc
            
            self.stats['deduplicated'] += dedup_count
            logger.info(f"{source_name}: æå– {count} æ¡, å»é‡ {dedup_count} æ¡")
            self.stats['sources_processed'] += 1
            
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥ {source_name}: {e}")
    
    def load_existing_db(self):
        existing_path = self.config.get('existing_db')
        
        if existing_path and existing_path.startswith(('http://', 'https://')):
            try:
                logger.info(f"ä»URLåŠ è½½ç°æœ‰æ•°æ®åº“...")
                response = requests.get(existing_path, timeout=30)
                if response.status_code == 200:
                    self.desc_db = response.json()
                    total = sum(len(v) for v in self.desc_db.values())
                    logger.info(f"åŠ è½½ç°æœ‰æ•°æ®åº“: {total} æ¡è®°å½•")
                    return
            except Exception as e:
                logger.warning(f"ä»URLåŠ è½½å¤±è´¥: {e}")
        
        if os.path.exists(self.output_path):
            try:
                with open(self.output_path, 'r', encoding='utf-8') as f:
                    self.desc_db = json.load(f)
                total = sum(len(v) for v in self.desc_db.values())
                logger.info(f"åŠ è½½æœ¬åœ°æ•°æ®åº“: {total} æ¡è®°å½•")
            except Exception as e:
                logger.warning(f"åŠ è½½æœ¬åœ°æ•°æ®åº“å¤±è´¥: {e}")
                self.desc_db = {}
        else:
            logger.info("æ— ç°æœ‰æ•°æ®åº“ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
    
    def save_database(self):
    os.makedirs(os.path.dirname(self.output_path) or '.', exist_ok=True)
    
    self.stats['channels_with_desc'] = len(self.desc_db)
    self.stats['total_descs'] = sum(len(v) for v in self.desc_db.values())
    
    with open(self.output_path, 'w', encoding='utf-8') as f:
        json.dump(self.desc_db, f, ensure_ascii=False, indent=2)  # â† æ”¹è¿™é‡Œ
    
    file_size = os.path.getsize(self.output_path)
    size_str = f"{file_size / 1024:.1f}KB" if file_size < 1024*1024 else f"{file_size / 1024 / 1024:.1f}MB"
    
    logger.info(f"æ•°æ®åº“å·²ä¿å­˜: {self.output_path} ({size_str})")
    logger.info(f"é¢‘é“æ•°: {self.stats['channels_with_desc']}, Descæ€»æ•°: {self.stats['total_descs']}")
    logger.info(f"HTMLä¿®å¤: {self.stats['html_fixed']}, æ— æ•ˆè¿‡æ»¤: {self.stats['invalid_filtered']}")
    
    def save_log(self):
        log_path = self.output_path.replace('.json', '_log.txt')
        now = datetime.now(BEIJING_TZ)
        
        with open(log_path, 'w', encoding='utf-8') as f:
            f.write(f"EPG Desc æå–æ—¥å¿— - {now.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 50 + "\n\n")
            
            f.write("ğŸ“Š ç»Ÿè®¡\n")
            f.write("-" * 30 + "\n")
            f.write(f"ç›®æ ‡é¢‘é“æ•°: {len(set(self.target_channels.values()))}\n")
            f.write(f"å¤„ç†EPGæºæ•°: {self.stats['sources_processed']}\n")
            f.write(f"æ–°å¢descæ•°: {self.stats['new_descs']}\n")
            f.write(f"å»é‡èŠ‚ç›®æ•°: {self.stats['deduplicated']}\n")
            f.write(f"HTMLä¿®å¤æ•°: {self.stats['html_fixed']}\n")
            f.write(f"æ— æ•ˆè¿‡æ»¤æ•°: {self.stats['invalid_filtered']}\n")
            f.write(f"æ€»descæ•°: {self.stats['total_descs']}\n")
            f.write(f"è¦†ç›–é¢‘é“æ•°: {self.stats['channels_with_desc']}\n\n")
            
            f.write("ğŸ“º å„é¢‘é“descæ•°é‡\n")
            f.write("-" * 30 + "\n")
            
            channel_counts = []
            for norm_ch, programs in self.desc_db.items():
                if programs:
                    sample = list(programs.values())[0]
                    channel_name = sample.get('channel', norm_ch)
                    channel_counts.append((channel_name, len(programs)))
            
            channel_counts.sort(key=lambda x: -x[1])
            for name, count in channel_counts:
                f.write(f"{name}: {count}\n")
        
        logger.info(f"æ—¥å¿—å·²ä¿å­˜: {log_path}")
    
    def run(self):
        logger.info("å¼€å§‹æå–Desc")
        
        self.load_config()
        
        if not self.load_target_channels():
            return False
        
        if self.config.get('accumulate', True):
            self.load_existing_db()
        
        for source in self.config.get('desc_sources', []):
            self.extract_from_source(source, source.get('name', 'unknown'))
        
        self.save_database()
        self.save_log()
        
        logger.info("Descæå–å®Œæˆ")
        return True


def main():
    parser = argparse.ArgumentParser(description='EPG Descæå–å™¨')
    parser.add_argument('--config', required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    extractor = DescExtractor(args.config, args.output)
    success = extractor.run()
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
