#!/usr/bin/env python3
"""
EPG Desc æå–å™¨
ä»å¤šä¸ªEPGæºæå–descï¼Œä¿å­˜ä¸ºJSONæ ¼å¼
ä¼˜åŒ–ï¼šåªå–channelçš„ç¬¬ä¸€ä¸ªdisplay-nameä½œä¸ºé¢‘é“å
"""
import os
import sys
import json
import logging
import argparse
import gzip
import xml.etree.ElementTree as ET
from datetime import datetime, timezone, timedelta
import requests

BEIJING_TZ = timezone(timedelta(hours=8))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DescExtractor:
    def __init__(self, config_path, output_path):
        self.config_path = config_path
        self.output_path = output_path
        self.config = None
        
        # ç›®æ ‡é¢‘é“: {normalized_name: canonical_name}
        # canonical_name æ˜¯ç¬¬ä¸€ä¸ªdisplay-name
        self.target_channels = {}
        
        # descæ•°æ®åº“: {norm_channel: {norm_title: {"channel": str, "title": str, "desc": str}}}
        self.desc_db = {}
        
        self.stats = {
            'sources_processed': 0,
            'total_descs': 0,
            'new_descs': 0,
            'channels_with_desc': 0
        }
    
    def normalize(self, text):
        """æ ‡å‡†åŒ–æ–‡æœ¬ç”¨äºåŒ¹é…"""
        if not text:
            return ""
        import re
        text = re.sub(r'[\s\-_\+\|\(\)ï¼ˆï¼‰\[\]ã€ã€‘ã€Šã€‹]', '', text)
        return text.lower()
    
    def load_config(self):
        """åŠ è½½é…ç½®"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        logger.info("é…ç½®åŠ è½½å®Œæˆ")
    
    def download_epg(self, url, compressed=True):
        """ä¸‹è½½EPG"""
        try:
            if url.startswith(('http://', 'https://')):
                logger.info(f"ä¸‹è½½: {url[:80]}...")
                response = requests.get(url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }, timeout=120)
                response.raise_for_status()
                content = response.content
            else:
                with open(url, 'rb') as f:
                    content = f.read()
            
            if compressed:
                try:
                    content = gzip.decompress(content)
                except:
                    pass
            
            return content
        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            return None
    
    def load_target_channels(self):
        """
        ä»å‚è€ƒEPGåŠ è½½ç›®æ ‡é¢‘é“åˆ—è¡¨
        åªå–æ¯ä¸ªchannelçš„ç¬¬ä¸€ä¸ªdisplay-nameä½œä¸ºæ ‡å‡†åç§°
        ä½†æ‰€æœ‰display-nameéƒ½ä½œä¸ºåŒ¹é…åˆ«å
        """
        ref_epg = self.config.get('reference_epg')
        if not ref_epg:
            logger.error("é…ç½®ä¸­ç¼ºå°‘ reference_epg")
            return False
        
        content = self.download_epg(ref_epg['url'], ref_epg.get('compressed', True))
        if not content:
            return False
        
        try:
            root = ET.fromstring(content)
            
            for channel in root.findall('.//channel'):
                display_names = channel.findall('display-name')
                if not display_names:
                    continue
                
                # ç¬¬ä¸€ä¸ªdisplay-nameä½œä¸ºæ ‡å‡†åç§°
                first_name = display_names[0].text.strip() if display_names[0].text else None
                if not first_name:
                    continue
                
                # æ‰€æœ‰display-nameéƒ½æ˜ å°„åˆ°ç¬¬ä¸€ä¸ªåç§°
                for dn in display_names:
                    if dn.text:
                        alias = dn.text.strip()
                        norm_alias = self.normalize(alias)
                        self.target_channels[norm_alias] = first_name
            
            unique_channels = len(set(self.target_channels.values()))
            logger.info(f"ç›®æ ‡é¢‘é“æ•°: {unique_channels}, åˆ«åæ€»æ•°: {len(self.target_channels)}")
            return True
            
        except Exception as e:
            logger.error(f"è§£æå‚è€ƒEPGå¤±è´¥: {e}")
            return False
    
    def get_canonical_channel(self, channel_name):
        """
        è·å–æ ‡å‡†é¢‘é“å
        è¿”å›: (æ ‡å‡†åç§°, æ˜¯å¦ä¸ºç›®æ ‡é¢‘é“)
        """
        norm = self.normalize(channel_name)
        if norm in self.target_channels:
            return self.target_channels[norm], True
        return channel_name, False
    
    def extract_from_source(self, epg_config, source_name):
        """ä»å•ä¸ªæºæå–desc"""
        content = self.download_epg(epg_config['url'], epg_config.get('compressed', True))
        if not content:
            return
        
        try:
            root = ET.fromstring(content)
            
            # å»ºç«‹channel_idåˆ°ç¬¬ä¸€ä¸ªdisplay-nameçš„æ˜ å°„
            channel_map = {}
            for ch in root.findall('.//channel'):
                cid = ch.get('id')
                display_names = ch.findall('display-name')
                if display_names and display_names[0].text:
                    channel_map[cid] = display_names[0].text.strip()
            
            count = 0
            for prog in root.findall('.//programme'):
                cid = prog.get('channel')
                source_channel_name = channel_map.get(cid, '')
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡é¢‘é“ï¼Œå¹¶è·å–æ ‡å‡†åç§°
                canonical_name, is_target = self.get_canonical_channel(source_channel_name)
                if not is_target:
                    continue
                
                title_elem = prog.find('title')
                desc_elem = prog.find('desc')
                
                if title_elem is None or not title_elem.text:
                    continue
                if desc_elem is None or not desc_elem.text:
                    continue
                
                title = title_elem.text.strip()
                desc = desc_elem.text.strip()
                
                if not desc or len(desc) < 5:  # è¿‡æ»¤å¤ªçŸ­çš„desc
                    continue
                
                norm_channel = self.normalize(canonical_name)
                norm_title = self.normalize(title)
                
                if norm_channel not in self.desc_db:
                    self.desc_db[norm_channel] = {}
                
                # åªä¿ç•™ç¬¬ä¸€ä¸ªï¼Œæˆ–æ›´é•¿çš„desc
                if norm_title not in self.desc_db[norm_channel]:
                    self.desc_db[norm_channel][norm_title] = {
                        "channel": canonical_name,
                        "title": title,
                        "desc": desc
                    }
                    count += 1
                    self.stats['new_descs'] += 1
                elif len(desc) > len(self.desc_db[norm_channel][norm_title]["desc"]):
                    # æ›´é•¿çš„descæ›¿æ¢çŸ­çš„
                    self.desc_db[norm_channel][norm_title]["desc"] = desc
            
            logger.info(f"{source_name}: æå– {count} æ¡æ–°desc")
            self.stats['sources_processed'] += 1
            
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥ {source_name}: {e}")
    
    def load_existing_db(self):
        """åŠ è½½ç°æœ‰æ•°æ®åº“ï¼ˆç´¯åŠ æ¨¡å¼ï¼‰"""
        existing_path = self.config.get('existing_db')
        
        # ä¼˜å…ˆä»é…ç½®çš„URLåŠ è½½
        if existing_path and existing_path.startswith(('http://', 'https://')):
            try:
                logger.info(f"ä»URLåŠ è½½ç°æœ‰æ•°æ®åº“...")
                response = requests.get(existing_path, timeout=30)
                if response.status_code == 200:
                    self.desc_db = response.json()
                    total = sum(len(v) for v in self.desc_db.values())
                    logger.info(f"åŠ è½½ç°æœ‰æ•°æ®åº“: {total} æ¡è®°å½•")
                    return
            except Exception as e:
                logger.warning(f"ä»URLåŠ è½½å¤±è´¥: {e}")
        
        # å…¶æ¬¡ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
        if os.path.exists(self.output_path):
            try:
                with open(self.output_path, 'r', encoding='utf-8') as f:
                    self.desc_db = json.load(f)
                total = sum(len(v) for v in self.desc_db.values())
                logger.info(f"åŠ è½½æœ¬åœ°æ•°æ®åº“: {total} æ¡è®°å½•")
            except Exception as e:
                logger.warning(f"åŠ è½½æœ¬åœ°æ•°æ®åº“å¤±è´¥: {e}")
                self.desc_db = {}
        else:
            logger.info("æ— ç°æœ‰æ•°æ®åº“ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
    
    def save_database(self):
        """ä¿å­˜æ•°æ®åº“"""
        os.makedirs(os.path.dirname(self.output_path) or '.', exist_ok=True)
        
        # ç»Ÿè®¡
        self.stats['channels_with_desc'] = len(self.desc_db)
        self.stats['total_descs'] = sum(len(v) for v in self.desc_db.values())
        
        # ä¿å­˜ä¸ºç´§å‡‘JSON
        with open(self.output_path, 'w', encoding='utf-8') as f:
            json.dump(self.desc_db, f, ensure_ascii=False, separators=(',', ':'))
        
        # è®¡ç®—æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(self.output_path)
        size_str = f"{file_size / 1024:.1f}KB" if file_size < 1024*1024 else f"{file_size / 1024 / 1024:.1f}MB"
        
        logger.info(f"æ•°æ®åº“å·²ä¿å­˜: {self.output_path} ({size_str})")
        logger.info(f"é¢‘é“æ•°: {self.stats['channels_with_desc']}, Descæ€»æ•°: {self.stats['total_descs']}")
    
    def save_log(self):
        """ä¿å­˜æå–æ—¥å¿—"""
        log_path = self.output_path.replace('.json', '_log.txt')
        now = datetime.now(BEIJING_TZ)
        
        with open(log_path, 'w', encoding='utf-8') as f:
            f.write(f"EPG Desc æå–æ—¥å¿— - {now.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 50 + "\n\n")
            
            f.write("ğŸ“Š ç»Ÿè®¡\n")
            f.write("-" * 30 + "\n")
            f.write(f"ç›®æ ‡é¢‘é“æ•°: {len(set(self.target_channels.values()))}\n")
            f.write(f"å¤„ç†EPGæºæ•°: {self.stats['sources_processed']}\n")
            f.write(f"æ–°å¢descæ•°: {self.stats['new_descs']}\n")
            f.write(f"æ€»descæ•°: {self.stats['total_descs']}\n")
            f.write(f"è¦†ç›–é¢‘é“æ•°: {self.stats['channels_with_desc']}\n\n")
            
            # åˆ—å‡ºæ¯ä¸ªé¢‘é“çš„descæ•°é‡
            f.write("ğŸ“º å„é¢‘é“descæ•°é‡\n")
            f.write("-" * 30 + "\n")
            
            channel_counts = []
            for norm_ch, programs in self.desc_db.items():
                if programs:
                    # è·å–æ ‡å‡†é¢‘é“å
                    sample = list(programs.values())[0]
                    channel_name = sample.get('channel', norm_ch)
                    channel_counts.append((channel_name, len(programs)))
            
            # æŒ‰æ•°é‡æ’åº
            channel_counts.sort(key=lambda x: -x[1])
            for name, count in channel_counts:
                f.write(f"{name}: {count}\n")
        
        logger.info(f"æ—¥å¿—å·²ä¿å­˜: {log_path}")
    
    def run(self):
        """è¿è¡Œæå–æµç¨‹"""
        logger.info("å¼€å§‹æå–Desc")
        
        self.load_config()
        
        if not self.load_target_channels():
            return False
        
        # åŠ è½½ç°æœ‰æ•°æ®åº“ï¼ˆç´¯åŠ æ¨¡å¼ï¼‰
        if self.config.get('accumulate', True):
            self.load_existing_db()
        
        # ä»å„æºæå–
        for source in self.config.get('desc_sources', []):
            self.extract_from_source(source, source.get('name', 'unknown'))
        
        self.save_database()
        self.save_log()
        
        logger.info("Descæå–å®Œæˆ")
        return True


def main():
    parser = argparse.ArgumentParser(description='EPG Descæå–å™¨')
    parser.add_argument('--config', required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    extractor = DescExtractor(args.config, args.output)
    success = extractor.run()
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
